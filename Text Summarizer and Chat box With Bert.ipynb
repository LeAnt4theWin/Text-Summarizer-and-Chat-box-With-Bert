{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957bcb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bcc9ba5d7645a7b7c06530ef3fca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe300e9ff64aa9bb9cb322b10c40a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaadb59adcec473ba91040ae4a563e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24a63237124189acb88e926aaf0050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fc97ecd89e49f1af7b51ce8a69e996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            yield page.get_text()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Patterns to remove from text\n",
    "    patterns_to_remove = [\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "        r'http[s]?://\\S+',  # URLs\n",
    "        r'\\S*studocu\\S*',  # studocu specific text\n",
    "        r'Downloaded by [\\w\\s\\']+ \\(\\S+\\)',  # Download footers\n",
    "        r'[\\u2022\\u2023\\u25E6\\u2043\\u2219\\-â€¢*]',  # Bullet points\n",
    "        r'\\bPage \\d+ of \\d+\\b',  # Page numbers\n",
    "        r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}',  # Dates\n",
    "    ]\n",
    "    combined_pattern = '|'.join(patterns_to_remove)\n",
    "    cleaned_text = re.sub(combined_pattern, '', text)\n",
    "\n",
    "    # Remove characters not representable in Latin-1 encoding\n",
    "    cleaned_text = cleaned_text.encode('latin-1', 'ignore').decode('latin-1')\n",
    "    \n",
    "    return cleaned_text\n",
    "# Initialize the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_pdf_to_pdf(pdf_path, output_pdf_path):\n",
    "    # Initialize a list to collect summaries\n",
    "    summaries = []\n",
    "    \n",
    "    for page_num, text in enumerate(extract_text_from_pdf(pdf_path), start=1):\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        \n",
    "        if not cleaned_text.strip():\n",
    "            summaries.append(f\"Page {page_num} is empty or contains only removable patterns.\")\n",
    "        else:\n",
    "            # Summarize the cleaned text\n",
    "            summary = summarizer(cleaned_text, max_length=6000, min_length=150, do_sample=False)\n",
    "            summaries.append(f\"Page {page_num} Summary:\\n{summary[0]['summary_text']}\\n\")\n",
    "    \n",
    "    # Initialize PDF writer\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    # Write each summary to the PDF\n",
    "    for summary in summaries:\n",
    "        pdf.add_page()\n",
    "        pdf.multi_cell(0, 10, summary)\n",
    "    \n",
    "    # Save the PDF to a file\n",
    "    pdf.output(output_pdf_path)\n",
    "def aiChatBox():\n",
    "    \n",
    "    userQuestion = input(\"Ask me anything\")\n",
    "    input_dict = tokenizer.prepare_seq2seq_batch(userQuestion, return_tensors=\"pt\") \n",
    "    generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "    print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd3969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf72d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 6000, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pdf_path = r\"\\Users\\leant\\Downloads\\chapter-2-the-biology-of-the-mind.pdf\"\n",
    "summarize_pdf_to_pdf(pdf_path, \"summary_output.pdf\")\n",
    "print(\"Summarization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ce955e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me anythingWho is the US president in 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\rag\\tokenization_rag.py:92: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "C:\\Users\\leant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_utils.py:1972: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " donald trump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_logits_process.py:489: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  torch.sparse.LongTensor(banned_mask.t(), indices, scores.size())\n"
     ]
    }
   ],
   "source": [
    "aiChatBox()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
